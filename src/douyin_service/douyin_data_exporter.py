"""
æŠ–éŸ³æ•°æ®å¯¼å‡ºæ¨¡å—
ç”¨äºå°†æå–çš„æŠ–éŸ³æ•°æ®å¯¼å‡ºä¸ºExcelå’ŒWordæ ¼å¼
"""

import pandas as pd
from docx import Document
from docx.shared import Pt
from docx.enum.text import WD_PARAGRAPH_ALIGNMENT
from datetime import datetime
import os
import re
from typing import List, Dict, Any
from loguru import logger


class DouyinDataExporter:
    """æŠ–éŸ³æ•°æ®å¯¼å‡ºå™¨"""
    
    def __init__(self, output_dir: str = "/Users/Zhuanz/projects/PythonWS/Alipay/data"):
        """
        åˆå§‹åŒ–æ•°æ®å¯¼å‡ºå™¨
        
        Args:
            output_dir: è¾“å‡ºç›®å½•è·¯å¾„
        """
        self.output_dir = output_dir
        self.douyin_data_dir = os.path.join(output_dir, "douyin_data")
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(self.douyin_data_dir, exist_ok=True)
        
    def parse_douyin_data(self, stats_file: str = "/Users/Zhuanz/projects/PythonWS/Alipay/3.txt", 
                         content_file: str = "/Users/Zhuanz/projects/PythonWS/Alipay/2.txt") -> List[Dict[str, Any]]:
        """
        è§£ææŠ–éŸ³æ•°æ®æ–‡ä»¶ï¼Œæ•´åˆç»Ÿè®¡æ•°æ®å’Œæ–‡æ¡ˆå†…å®¹
        
        Args:
            stats_file: ç»Ÿè®¡æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆ3.txtï¼‰
            content_file: æ–‡æ¡ˆå†…å®¹æ–‡ä»¶è·¯å¾„ï¼ˆ2.txtï¼‰
            
        Returns:
            List[Dict]: æ•´åˆåçš„æ•°æ®åˆ—è¡¨
        """
        try:
            # è¯»å–ç»Ÿè®¡æ•°æ®æ–‡ä»¶
            stats_data = self._parse_stats_file(stats_file)
            
            # è¯»å–æ–‡æ¡ˆå†…å®¹æ–‡ä»¶
            content_data = self._parse_content_file(content_file)
            
            # æ•´åˆæ•°æ®
            merged_data = []
            for stats in stats_data:
                video_url = stats.get("video_url", "")
                # ä»æ–‡æ¡ˆæ–‡ä»¶ä¸­æ‰¾åˆ°å¯¹åº”çš„å†…å®¹
                content = self._find_content_by_url(video_url, content_data)
                
                merged_item = {
                    "video_url": video_url,
                    "content_text": content.get("content_text", ""),
                    "publish_time": stats.get("publish_time", ""),
                    "like_count": stats.get("like_count", 0),
                    "comment_count": stats.get("comment_count", 0),
                    "collect_count": stats.get("collect_count", 0),
                    "share_count": stats.get("share_count", 0),
                    "publish_time_parsed": self._parse_publish_time(stats.get("publish_time", ""))
                }
                
                merged_data.append(merged_item)
            
            logger.info(f"âœ… æˆåŠŸè§£ææŠ–éŸ³æ•°æ®ï¼Œå…± {len(merged_data)} æ¡è®°å½•")
            return merged_data
            
        except Exception as e:
            logger.error(f"è§£ææŠ–éŸ³æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            raise
    
    def _parse_stats_file(self, file_path: str) -> List[Dict[str, Any]]:
        """è§£æç»Ÿè®¡æ•°æ®æ–‡ä»¶ï¼ˆ3.txtï¼‰"""
        stats_data = []
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # æŒ‰è§†é¢‘åˆ†å‰²å†…å®¹
            video_sections = re.split(r'=== è§†é¢‘URL: (.*?) ===', content)
            
            for i in range(1, len(video_sections), 2):
                if i + 1 < len(video_sections):
                    url = video_sections[i].strip()
                    section_content = video_sections[i + 1]
                    
                    # æå–å„é¡¹æ•°æ®
                    stats = {
                        "video_url": url,
                        "like_count": self._extract_number_from_text(section_content, "ç‚¹èµæ•°"),
                        "comment_count": self._extract_number_from_text(section_content, "è¯„è®ºæ•°"),
                        "collect_count": self._extract_number_from_text(section_content, "æ”¶è—æ•°"),
                        "share_count": self._extract_number_from_text(section_content, "è½¬å‘æ•°"),
                        "publish_time": self._extract_publish_time(section_content)
                    }
                    
                    stats_data.append(stats)
            
            logger.info(f"ä»ç»Ÿè®¡æ•°æ®æ–‡ä»¶ä¸­è§£æå‡º {len(stats_data)} æ¡è®°å½•")
            
        except Exception as e:
            logger.error(f"è§£æç»Ÿè®¡æ•°æ®æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            
        return stats_data
    
    def _parse_content_file(self, file_path: str) -> List[Dict[str, Any]]:
        """è§£ææ–‡æ¡ˆå†…å®¹æ–‡ä»¶ï¼ˆ2.txtï¼‰"""
        content_data = []
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()
            
            current_url = ""
            current_content = []
            
            for line in lines:
                line = line.strip()
                if line.startswith('https://www.douyin.com/video/'):
                    # ä¿å­˜å‰ä¸€ä¸ªè§†é¢‘çš„å†…å®¹
                    if current_url and current_content:
                        content_data.append({
                            "video_url": current_url,
                            "content_text": "\n".join(current_content).strip()
                        })
                    
                    # å¼€å§‹æ–°çš„è§†é¢‘
                    current_url = line
                    current_content = []
                elif line and current_url:
                    current_content.append(line)
            
            # ä¿å­˜æœ€åä¸€ä¸ªè§†é¢‘çš„å†…å®¹
            if current_url and current_content:
                content_data.append({
                    "video_url": current_url,
                    "content_text": "\n".join(current_content).strip()
                })
            
            logger.info(f"ä»æ–‡æ¡ˆå†…å®¹æ–‡ä»¶ä¸­è§£æå‡º {len(content_data)} æ¡è®°å½•")
            
        except Exception as e:
            logger.error(f"è§£ææ–‡æ¡ˆå†…å®¹æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            
        return content_data
    
    def _find_content_by_url(self, url: str, content_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """æ ¹æ®URLæŸ¥æ‰¾å¯¹åº”çš„æ–‡æ¡ˆå†…å®¹"""
        for content in content_data:
            if content.get("video_url", "") == url:
                return content
        return {"content_text": ""}
    
    def _extract_number_from_text(self, text: str, field_name: str) -> int:
        """ä»æ–‡æœ¬ä¸­æå–æŒ‡å®šå­—æ®µçš„æ•°å€¼"""
        try:
            pattern = f"{field_name}: (\\d+)"
            match = re.search(pattern, text)
            if match:
                return int(match.group(1))
        except Exception:
            pass
        return 0
    
    def _extract_publish_time(self, text: str) -> str:
        """ä»æ–‡æœ¬ä¸­æå–å‘å¸ƒæ—¶é—´"""
        try:
            pattern = r"å‘å¸ƒæ—¶é—´: (.*?)\n"
            match = re.search(pattern, text)
            if match:
                return match.group(1).strip()
        except Exception:
            pass
        return ""
    
    def _parse_publish_time(self, publish_time: str) -> str:
        """è§£æå‘å¸ƒæ—¶é—´ä¸ºæ ‡å‡†æ ¼å¼"""
        try:
            # å¤„ç†æ ¼å¼å¦‚ "å‘å¸ƒæ—¶é—´ï¼š2025-10-18 09:01"
            if "å‘å¸ƒæ—¶é—´ï¼š" in publish_time:
                time_str = publish_time.replace("å‘å¸ƒæ—¶é—´ï¼š", "").strip()
                # å°è¯•è§£æä¸ºdatetimeå¯¹è±¡
                dt = datetime.strptime(time_str, "%Y-%m-%d %H:%M")
                return dt.strftime("%Y-%m-%d")
        except Exception:
            pass
        return ""
    
    def export_to_excel(self, data: List[Dict[str, Any]], filename: str = "douyin_data.xlsx") -> str:
        """
        å°†æ•°æ®å¯¼å‡ºä¸ºExcelæ ¼å¼
        
        Args:
            data: è¦å¯¼å‡ºçš„æ•°æ®åˆ—è¡¨
            filename: è¾“å‡ºæ–‡ä»¶å
            
        Returns:
            str: è¾“å‡ºæ–‡ä»¶çš„å®Œæ•´è·¯å¾„
        """
        try:
            # å‡†å¤‡Excelæ•°æ®
            excel_data = []
            
            for item in data:
                excel_row = {
                    "è§†é¢‘URL": item.get("video_url", ""),
                    "æ–‡æ¡ˆå†…å®¹": item.get("content_text", ""),
                    "å‘å¸ƒæ—¶é—´": item.get("publish_time_parsed", ""),
                    "ç‚¹èµæ•°": item.get("like_count", 0),
                    "è¯„è®ºæ•°": item.get("comment_count", 0),
                    "æ”¶è—æ•°": item.get("collect_count", 0),
                    "è½¬å‘æ•°": item.get("share_count", 0),
                    "å¹³å°": "æŠ–éŸ³"
                }
                
                excel_data.append(excel_row)
            
            # åˆ›å»ºDataFrame
            df = pd.DataFrame(excel_data)
            
            # æŒ‰å‘å¸ƒæ—¶é—´å€’åºæ’åˆ—
            if not df.empty and 'å‘å¸ƒæ—¶é—´' in df.columns:
                df = df.sort_values('å‘å¸ƒæ—¶é—´', ascending=False)
            
            # ä¿å­˜åˆ°Excelæ–‡ä»¶
            output_path = os.path.join(self.douyin_data_dir, filename)
            df.to_excel(output_path, index=False, engine='openpyxl')
            
            logger.info(f"âœ… Excelæ–‡ä»¶å·²å¯¼å‡º: {output_path}")
            logger.info(f"ğŸ“Š å…±å¯¼å‡º {len(excel_data)} æ¡æ•°æ®")
            
            return output_path
            
        except Exception as e:
            logger.error(f"å¯¼å‡ºExcelæ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            raise
    
    def export_to_word(self, data: List[Dict[str, Any]], filename: str = "douyin_content.docx") -> str:
        """
        å°†æ•°æ®å¯¼å‡ºä¸ºWordæ ¼å¼ï¼ŒæŒ‰é¡ºåºæ‘†æ”¾ï¼Œåªä¿ç•™æ—¶é—´å’Œå†…å®¹
        
        Args:
            data: è¦å¯¼å‡ºçš„æ•°æ®åˆ—è¡¨
            filename: è¾“å‡ºæ–‡ä»¶å
            
        Returns:
            str: è¾“å‡ºæ–‡ä»¶çš„å®Œæ•´è·¯å¾„
        """
        try:
            # åˆ›å»ºWordæ–‡æ¡£
            doc = Document()
            
            # è®¾ç½®æ ‡é¢˜
            title = doc.add_heading('æ”¯ä»˜å®æŠ–éŸ³è§†é¢‘å†…å®¹æ±‡æ€»', 0)
            title.alignment = WD_PARAGRAPH_ALIGNMENT.CENTER
            
            # æŒ‰å‘å¸ƒæ—¶é—´å€’åºæ’åˆ—æ•°æ®
            sorted_data = sorted(data, 
                              key=lambda x: x.get("publish_time_parsed", ""), 
                              reverse=True)
            
            # æ·»åŠ æ¯æ¡å†…å®¹ï¼ˆåªä¿ç•™æ—¶é—´å’Œå†…å®¹ï¼‰
            for item in sorted_data:
                # å‘å¸ƒæ—¶é—´
                publish_time = item.get("publish_time", "")
                if publish_time:
                    time_para = doc.add_paragraph()
                    time_run = time_para.add_run(f"ã€å‘å¸ƒæ—¶é—´ã€‘{publish_time}")
                    time_run.font.size = Pt(11)
                    time_run.bold = True
                
                # æ–‡æ¡ˆå†…å®¹
                content_text = item.get("content_text", "")
                if content_text:
                    text_para = doc.add_paragraph()
                    text_run = text_para.add_run(f"ã€æ–‡æ¡ˆå†…å®¹ã€‘{content_text}")
                    text_run.font.size = Pt(11)
                
                # æ·»åŠ ç©ºè¡Œåˆ†éš”
                doc.add_paragraph()
            
            # ä¿å­˜Wordæ–‡æ¡£
            output_path = os.path.join(self.douyin_data_dir, filename)
            doc.save(output_path)
            
            logger.info(f"âœ… Wordæ–‡æ¡£å·²å¯¼å‡º: {output_path}")
            logger.info(f"ğŸ“„ å…±å¯¼å‡º {len(data)} æ¡å†…å®¹")
            
            return output_path
            
        except Exception as e:
            logger.error(f"å¯¼å‡ºWordæ–‡æ¡£æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            raise
    
    def _group_by_month(self, data: List[Dict[str, Any]]) -> Dict[str, List[Dict[str, Any]]]:
        """
        æŒ‰æœˆä»½åˆ†ç»„æ•°æ®
        
        Args:
            data: æ•°æ®åˆ—è¡¨
            
        Returns:
            Dict: æŒ‰æœˆä»½åˆ†ç»„çš„æ•°æ®å­—å…¸
        """
        monthly_data = {}
        
        for item in data:
            publish_date = item.get("publish_time_parsed", "")
            if publish_date:
                try:
                    # è§£ææ—¥æœŸ
                    date_obj = datetime.strptime(publish_date, "%Y-%m-%d")
                    month_key = f"{date_obj.year}å¹´{date_obj.month}æœˆ"
                    
                    if month_key not in monthly_data:
                        monthly_data[month_key] = []
                    
                    monthly_data[month_key].append(item)
                    
                except ValueError:
                    logger.warning(f"æ— æ³•è§£ææ—¥æœŸ: {publish_date}")
                    continue
            else:
                # æ²¡æœ‰æ—¥æœŸçš„æ•°æ®æ”¾å…¥"æœªçŸ¥æ—¶é—´"åˆ†ç»„
                if "æœªçŸ¥æ—¶é—´" not in monthly_data:
                    monthly_data["æœªçŸ¥æ—¶é—´"] = []
                monthly_data["æœªçŸ¥æ—¶é—´"].append(item)
        
        return monthly_data
    
    def export_all_formats(self, data: List[Dict[str, Any]]) -> Dict[str, str]:
        """
        å¯¼å‡ºæ‰€æœ‰æ ¼å¼çš„æ–‡ä»¶
        
        Args:
            data: è¦å¯¼å‡ºçš„æ•°æ®åˆ—è¡¨
            
        Returns:
            Dict: åŒ…å«å„æ–‡ä»¶è·¯å¾„çš„å­—å…¸
        """
        results = {}
        
        try:
            # å¯¼å‡ºExcel
            excel_path = self.export_to_excel(data)
            results["excel"] = excel_path
            
            # å¯¼å‡ºWord
            word_path = self.export_to_word(data)
            results["word"] = word_path
            
            logger.info("âœ… æ‰€æœ‰æ ¼å¼æ–‡ä»¶å¯¼å‡ºå®Œæˆ")
            
        except Exception as e:
            logger.error(f"å¯¼å‡ºæ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            raise
        
        return results


# éœ€è¦å¯¼å…¥RGBColorç”¨äºè®¾ç½®å­—ä½“é¢œè‰²
from docx.shared import RGBColor